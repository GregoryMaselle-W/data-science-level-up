{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10b73e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyopencl.Platform 'Intel(R) OpenCL Graphics' at 0x29570ae73c0>,\n",
       " <pyopencl.Platform 'NVIDIA CUDA' at 0x2959abe9710>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyopencl as cl\n",
    "cl.get_platforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4bc0b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Column FAILURE_CODE has mixed types: {<class 'int'>, <class 'str'>}. Making it all string\n",
      "Oversampling training set...\n",
      "Training set shape: (1655371, 170), Validation set shape: (213288, 170)\n",
      "[LightGBM] [Info] Number of positive: 824270, number of negative: 831101\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 37039\n",
      "[LightGBM] [Info] Number of data points in the train set: 1655371, number of used features: 167\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX A500 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 39 dense feature groups (63.15 MB) transferred to GPU in 0.044080 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497937 -> initscore=-0.008253\n",
      "[LightGBM] [Info] Start training from score -0.008253\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.990688\tvalid_0's binary_logloss: 0.241927\n",
      "[100]\tvalid_0's auc: 0.99162\tvalid_0's binary_logloss: 0.110587\n",
      "[150]\tvalid_0's auc: 0.992249\tvalid_0's binary_logloss: 0.0612584\n",
      "[200]\tvalid_0's auc: 0.992619\tvalid_0's binary_logloss: 0.0414452\n",
      "[250]\tvalid_0's auc: 0.992878\tvalid_0's binary_logloss: 0.0329417\n",
      "[300]\tvalid_0's auc: 0.993062\tvalid_0's binary_logloss: 0.0291359\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's auc: 0.993062\tvalid_0's binary_logloss: 0.0291359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbdnet3218\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9930615768380063\n",
      "PR AUC: 0.8941609609495906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     0.947     0.972    207775\n",
      "           1      0.326     0.961     0.486      5513\n",
      "\n",
      "    accuracy                          0.948    213288\n",
      "   macro avg      0.662     0.954     0.729    213288\n",
      "weighted avg      0.981     0.948     0.960    213288\n",
      "\n",
      "Confusion matrix:\n",
      " [[196808  10967]\n",
      " [   217   5296]]\n",
      "Processing test data...\n",
      "Column CARD_BRAND has mixed types: {<class 'str'>}. Making it all string\n",
      "Column TRANSACTION_TYPE has mixed types: {<class 'str'>}. Making it all string\n",
      "Column TRANSACTION_STATUS has mixed types: {<class 'str'>}. Making it all string\n",
      "Column FAILURE_CODE has mixed types: {<class 'int'>, <class 'str'>}. Making it all string\n",
      "Column FAILURE_REASON has mixed types: {<class 'str'>}. Making it all string\n",
      "Column TRANSACTION_CURRENCY has mixed types: {<class 'str'>}. Making it all string\n",
      "Column CARD_COUNTRY_CODE has mixed types: {<class 'str'>}. Making it all string\n",
      "Column IS_RECURRING_TRANSACTION has mixed types: {<class 'bool'>}. Making it all string\n",
      "Column CARDHOLDER_AUTH_METHOD has mixed types: {<class 'str'>}. Making it all string\n",
      "Column BUSINESS_TYPE has mixed types: {<class 'str'>}. Making it all string\n",
      "Column TAX_EXCEMPT_INDICATOR has mixed types: {<class 'bool'>}. Making it all string\n",
      "Column OUTLET_TYPE has mixed types: {<class 'str'>}. Making it all string\n",
      "Column DAY_OF_MONTH has mixed types: {<class 'int'>}. Making it all string\n",
      "Column WINDOW_AFTER_LAST_TRANSACTION_CATEGORY has mixed types: {<class 'str'>}. Making it all string\n",
      "Column EXPIRY_CATEGORY has mixed types: {<class 'str'>}. Making it all string\n",
      "Column TX_AMOUNT has mixed types: {<class 'float'>}. Making it all float\n",
      "Column TRANSACTION_GOODS_AND_SERVICES_AMOUNT has mixed types: {<class 'float'>}. Making it all float\n",
      "Column TRANSACTION_CASHBACK_AMOUNT has mixed types: {<class 'float'>}. Making it all float\n",
      "Column X_CUSTOMER_ID has mixed types: {<class 'float'>}. Making it all float\n",
      "Column Y_CUSTOMER_ID has mixed types: {<class 'float'>}. Making it all float\n",
      "Column Y_TERMINAL_ID has mixed types: {<class 'float'>}. Making it all float\n",
      "Column MCC_CODE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column ANNUAL_TURNOVER_CARD has mixed types: {<class 'float'>}. Making it all float\n",
      "Column ANNUAL_TURNOVER has mixed types: {<class 'float'>}. Making it all float\n",
      "Column AVERAGE_TICKET_SALE_AMOUNT has mixed types: {<class 'float'>}. Making it all float\n",
      "Column PAYMENT_PERCENTAGE_FACE_TO_FACE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column PAYMENT_PERCENTAGE_ECOM has mixed types: {<class 'float'>}. Making it all float\n",
      "Column PAYMENT_PERCENTAGE_MOTO has mixed types: {<class 'float'>}. Making it all float\n",
      "Column DEPOSIT_REQUIRED_PERCENTAGE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column DEPOSIT_PERCENTAGE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column DELIVERY_SAME_DAYS_PERCENTAGE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column DELIVERY_WEEK_ONE_PERCENTAGE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column DELIVERY_WEEK_TWO_PERCENTAGE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column DELIVERY_OVER_TWO_WEEKS_PERCENTAGE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column DAY_OF_WEEK has mixed types: {<class 'int'>}. Making it all float\n",
      "Column HOUR has mixed types: {<class 'int'>}. Making it all float\n",
      "Column IS_WEEKEND has mixed types: {<class 'int'>}. Making it all float\n",
      "Column TIME_SINCE_LAST_TRANSACTION has mixed types: {<class 'float'>}. Making it all float\n",
      "Column IS_FIRST_TRANSACTION has mixed types: {<class 'int'>}. Making it all float\n",
      "Column IS_BUSINESS_HOURS has mixed types: {<class 'int'>}. Making it all float\n",
      "Column MONTHS_UNTIL_EXPIRY has mixed types: {<class 'int'>}. Making it all float\n",
      "Column CUSTOMER_AVERAGE_AMOUNT has mixed types: {<class 'float'>}. Making it all float\n",
      "Column TX_AMOUNT_BY_CUSTOMER_AVG has mixed types: {<class 'float'>}. Making it all float\n",
      "Column NEAREST_FRAUD_DISTANCE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column AVG_3_NEAREST_FRAUDS has mixed types: {<class 'float'>}. Making it all float\n",
      "Column FRAUD_DENSITY_1KM has mixed types: {<class 'int'>}. Making it all float\n",
      "Column FRAUD_DENSITY_5KM has mixed types: {<class 'int'>}. Making it all float\n",
      "Column KNN_FRAUD_RISK_SCORE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column TERMINAL_FRAUD_RATE has mixed types: {<class 'float'>}. Making it all float\n",
      "Column TERMINAL_FRAUD_COUNT has mixed types: {<class 'int'>}. Making it all float\n",
      "Column TERMINAL_TX_COUNT has mixed types: {<class 'int'>}. Making it all float\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbdnet3218\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test predictions to predictions\\V0_LGBM.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, precision_recall_curve, roc_auc_score\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib\n",
    "\n",
    "# --------------------------\n",
    "# Globals for KNN and terminal stats\n",
    "# --------------------------\n",
    "terminal_stats = None\n",
    "fraud_locations = None\n",
    "\n",
    "# --------------------------\n",
    "# KNN helper\n",
    "# --------------------------\n",
    "def create_comprehensive_knn_features(df, fraud_coords, is_training=True):\n",
    "    if len(fraud_coords) < 3:\n",
    "        df['NEAREST_FRAUD_DISTANCE'] = 999.0\n",
    "        df['FRAUD_DENSITY_1KM'] = 0\n",
    "        df['FRAUD_DENSITY_5KM'] = 0\n",
    "        df['KNN_FRAUD_RISK_SCORE'] = 0.0\n",
    "        return df\n",
    "\n",
    "    all_coords = df[['X_CUSTOMER_ID','Y_CUSTOMER_ID']].values\n",
    "    knn_close = NearestNeighbors(n_neighbors=min(3,len(fraud_coords)), metric='euclidean').fit(fraud_coords)\n",
    "    knn_radius_1km = NearestNeighbors(radius=1.0, metric='euclidean').fit(fraud_coords)\n",
    "    knn_radius_5km = NearestNeighbors(radius=5.0, metric='euclidean').fit(fraud_coords)\n",
    "\n",
    "    distances,_ = knn_close.kneighbors(all_coords)\n",
    "    df['NEAREST_FRAUD_DISTANCE'] = distances[:,0]\n",
    "    df['AVG_3_NEAREST_FRAUDS'] = distances.mean(axis=1)\n",
    "\n",
    "    distances_1km,_ = knn_radius_1km.radius_neighbors(all_coords)\n",
    "    distances_5km,_ = knn_radius_5km.radius_neighbors(all_coords)\n",
    "    df['FRAUD_DENSITY_1KM'] = [len(idx) for idx in distances_1km]\n",
    "    df['FRAUD_DENSITY_5KM'] = [len(idx) for idx in distances_5km]\n",
    "\n",
    "    max_density_5km = df['FRAUD_DENSITY_5KM'].max()\n",
    "    df['KNN_FRAUD_RISK_SCORE'] = (\n",
    "        (1/(1+df['NEAREST_FRAUD_DISTANCE']))*0.4 +\n",
    "        (1/(1+df['AVG_3_NEAREST_FRAUDS']))*0.3 +\n",
    "        (df['FRAUD_DENSITY_5KM']/max_density_5km)*0.3\n",
    "    ) if max_density_5km>0 else 1/(1+df['NEAREST_FRAUD_DISTANCE'])\n",
    "    return df\n",
    "\n",
    "# --------------------------\n",
    "# Data processing function\n",
    "# --------------------------\n",
    "def process_data(filename, is_test=False):\n",
    "    global terminal_stats, fraud_locations\n",
    "\n",
    "    # Load CSVs\n",
    "    customers_df = pd.read_csv(\"customers.csv\", low_memory=False)\n",
    "    merchants_df = pd.read_csv(\"merchants.csv\", low_memory=False)\n",
    "    terminals_df = pd.read_csv(\"terminals.csv\", low_memory=False)\n",
    "    transactions_df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    # Fill missing values\n",
    "    transactions_df.fillna({'FAILURE_CODE': 0}, inplace=True)\n",
    "    transactions_df.fillna({'FAILURE_REASON': \"Approved or completed successfully\"}, inplace=True)\n",
    "    transactions_df.fillna({'CARDHOLDER_AUTH_METHOD': \"Online PIN\"}, inplace=True)\n",
    "\n",
    "    merged = transactions_df.merge(customers_df, on='CUSTOMER_ID', how='left')\\\n",
    "                            .merge(terminals_df, on='TERMINAL_ID', how='left')\\\n",
    "                            .merge(merchants_df, on='MERCHANT_ID', how='left')\n",
    "    merged.columns = merged.columns.str.upper()\n",
    "\n",
    "    # --------------------------\n",
    "    # Time features\n",
    "    # --------------------------\n",
    "    merged[\"TX_TS\"] = pd.to_datetime(merged[\"TX_TS\"])\n",
    "    merged[\"DAY_OF_WEEK\"] = merged['TX_TS'].dt.day_of_week\n",
    "    merged[\"HOUR\"] = merged['TX_TS'].dt.hour\n",
    "    merged[\"IS_WEEKEND\"] = merged['TX_TS'].dt.day_of_week >= 5\n",
    "    merged[\"DAY_OF_MONTH\"] = merged['TX_TS'].dt.day.astype(\"category\")\n",
    "\n",
    "    merged_sorted = merged.sort_values([\"CUSTOMER_ID\",\"TX_TS\"])\n",
    "    merged[\"TIME_SINCE_LAST_TRANSACTION\"] = merged_sorted.groupby(\"CUSTOMER_ID\")[\"TX_TS\"].diff().dt.total_seconds()\n",
    "    merged[\"IS_FIRST_TRANSACTION\"] = merged[\"TIME_SINCE_LAST_TRANSACTION\"].isna().astype(int)\n",
    "    merged[\"TIME_SINCE_LAST_TRANSACTION\"] = merged[\"TIME_SINCE_LAST_TRANSACTION\"].fillna(0)\n",
    "\n",
    "    def categorize_tx_freq(seconds):\n",
    "        if pd.isna(seconds): return \"first\"\n",
    "        elif seconds < 60: return \"under_1_min\"\n",
    "        elif seconds < 3600: return \"under_1_hour\"\n",
    "        elif seconds < 86400: return \"under_1_day\"\n",
    "        elif seconds < 604800: return \"under_1_week\"\n",
    "        elif seconds < 2.592e6: return \"under_1_month\"\n",
    "        elif seconds < 1.555e7: return \"under_6_months\"\n",
    "        else: return \"over_6_months\"\n",
    "\n",
    "    merged[\"WINDOW_AFTER_LAST_TRANSACTION_CATEGORY\"] = merged[\"TIME_SINCE_LAST_TRANSACTION\"].apply(categorize_tx_freq).astype(\"category\")\n",
    "    merged[\"IS_BUSINESS_HOURS\"] = ((merged[\"TX_TS\"].dt.hour >=8) & (merged[\"TX_TS\"].dt.hour <=17)).astype(int)\n",
    "\n",
    "    # --------------------------\n",
    "    # Card expiry\n",
    "    # --------------------------\n",
    "    merged[\"CARD_EXPIRY_DATE\"] = pd.to_datetime(\"01/\"+merged[\"CARD_EXPIRY_DATE\"].astype(str), format=\"%d/%m/%y\")\n",
    "    merged[\"CARD_EXPIRY_DATE\"] = merged[\"CARD_EXPIRY_DATE\"].dt.to_period('M').dt.end_time\n",
    "\n",
    "    def months_until_expiry(row):\n",
    "        return (row[\"CARD_EXPIRY_DATE\"].year - row[\"TX_TS\"].year) * 12 + (row[\"CARD_EXPIRY_DATE\"].month - row[\"TX_TS\"].month)\n",
    "    merged[\"MONTHS_UNTIL_EXPIRY\"] = merged.apply(months_until_expiry, axis=1)\n",
    "\n",
    "    def categorize_expiry(months):\n",
    "        if months <= 0: return \"expired\"\n",
    "        elif months < 1: return \"under_1_month\"\n",
    "        elif months < 3: return \"under_3_months\"\n",
    "        elif months < 6: return \"under_6_months\"\n",
    "        elif months < 12: return \"under_1_year\"\n",
    "        else: return \"over_1_year\"\n",
    "\n",
    "    merged[\"EXPIRY_CATEGORY\"] = merged[\"MONTHS_UNTIL_EXPIRY\"].apply(categorize_expiry).astype(\"category\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Customer avg amounts\n",
    "    # --------------------------\n",
    "    customer_avg = merged.groupby('CUSTOMER_ID')['TX_AMOUNT'].mean().reset_index().rename(columns={'TX_AMOUNT':'CUSTOMER_AVERAGE_AMOUNT'})\n",
    "    merged = merged.merge(customer_avg, on='CUSTOMER_ID', how='left')\n",
    "    merged['TX_AMOUNT_BY_CUSTOMER_AVG'] = merged['TX_AMOUNT']/merged['CUSTOMER_AVERAGE_AMOUNT']\n",
    "    merged.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    merged.fillna({'TX_AMOUNT_BY_CUSTOMER_AVG':0}, inplace=True)\n",
    "\n",
    "    # --------------------------\n",
    "    # KNN features\n",
    "    # --------------------------\n",
    "    if not is_test:\n",
    "        fraud_locations = merged[merged['TX_FRAUD']==1][['X_CUSTOMER_ID','Y_CUSTOMER_ID']].values\n",
    "        merged = create_comprehensive_knn_features(merged, fraud_locations, is_training=True)\n",
    "    else:\n",
    "        merged = create_comprehensive_knn_features(merged, fraud_locations, is_training=False)\n",
    "\n",
    "    # --------------------------\n",
    "    # Terminal stats\n",
    "    # --------------------------\n",
    "    if not is_test:\n",
    "        terminal_stats = merged.groupby(\"TERMINAL_ID\")[\"TX_FRAUD\"].agg([\"mean\",\"sum\",\"count\"]).reset_index()\\\n",
    "                              .rename(columns={\"mean\":\"TERMINAL_FRAUD_RATE\",\"sum\":\"TERMINAL_FRAUD_COUNT\",\"count\":\"TERMINAL_TX_COUNT\"})\n",
    "        merged = merged.merge(terminal_stats, on=\"TERMINAL_ID\", how=\"left\")\n",
    "    else:\n",
    "        merged = merged.merge(terminal_stats, on=\"TERMINAL_ID\", how=\"left\")\n",
    "        merged.fillna({\n",
    "            \"TERMINAL_FRAUD_RATE\":0,\n",
    "            \"TERMINAL_FRAUD_COUNT\":0,\n",
    "            \"TERMINAL_TX_COUNT\":0\n",
    "        }, inplace=True)\n",
    "\n",
    "    # --------------------------\n",
    "    # Drop unnecessary columns\n",
    "    # --------------------------\n",
    "    drop_cols = ['TX_ID','CARD_DATA','CUSTOMER_ID','TERMINAL_ID','MERCHANT_ID','TX_TS','CARD_EXPIRY_DATE','ACQUIRER_ID','LEGAL_NAME','X_TERMINAL_ID','Y_TERMINAL__ID']\n",
    "    merged = merged.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # --------------------------\n",
    "    # Categorical / bool\n",
    "    # --------------------------\n",
    "    categorical_cols = [\n",
    "        'CARD_BRAND', 'TRANSACTION_TYPE', 'TRANSACTION_STATUS',\n",
    "        'FAILURE_CODE', 'BUSINESS_TYPE', 'OUTLET_TYPE',\n",
    "        'DAY_OF_MONTH', 'WINDOW_AFTER_LAST_TRANSACTION_CATEGORY',\n",
    "        'EXPIRY_CATEGORY', 'DISTANCE_CATEGORY', 'CARD_COUNTRY_CODE',\n",
    "        'IS_RECURRING_TRANSACTION', 'CARDHOLDER_AUTH_METHOD',\n",
    "        'TRANSACTION_CURRENCY','FAILURE_REASON','TAX_EXCEMPT_INDICATOR'\n",
    "    ]\n",
    "    for col in categorical_cols:\n",
    "        if col in merged.columns:\n",
    "            merged[col] = merged[col].astype('category')\n",
    "\n",
    "    bool_cols = merged.select_dtypes(include='bool').columns\n",
    "    merged[bool_cols] = merged[bool_cols].astype(int)\n",
    "\n",
    "    return merged\n",
    "\n",
    "# --------------------------\n",
    "# Main\n",
    "# --------------------------\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "train_df = process_data(\"transactions_train.csv\", is_test=False)\n",
    "\n",
    "# Prepare X/y\n",
    "target_col = \"TX_FRAUD\"\n",
    "y = train_df[target_col].astype(int)\n",
    "X = train_df.drop(columns=[target_col])\n",
    "categorical = X.select_dtypes(\"category\").columns.tolist()\n",
    "numeric = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "for col in categorical:\n",
    "    types = set(type(x) for x in X[col].dropna())\n",
    "    if len(types) > 1:\n",
    "        print(f\"Column {col} has mixed types: {types}. Making it all string\")\n",
    "        X[col] = X[col].astype(str)\n",
    "\n",
    "for col in numeric:\n",
    "    types = set(type(x) for x in X[col].dropna())\n",
    "    if len(types) > 1:\n",
    "        print(f\"Column {col} has mixed types: {types}. Making it all float\")\n",
    "        X[col] = X[col].astype(float)\n",
    "\n",
    "# Train/validation split for evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# --------------------------\n",
    "# Preprocessing\n",
    "# --------------------------\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical),\n",
    "    (\"num\", \"passthrough\", numeric),\n",
    "])\n",
    "\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_val_t = preprocessor.transform(X_val)\n",
    "\n",
    "# --------------------------\n",
    "# Oversample training set\n",
    "# --------------------------\n",
    "print(\"Oversampling training set...\")\n",
    "adasyn = ADASYN(random_state=42, n_neighbors=5)\n",
    "X_train_os, y_train_os = adasyn.fit_resample(X_train_t, y_train)\n",
    "\n",
    "# --------------------------\n",
    "# Remove exact duplicates\n",
    "# --------------------------\n",
    "Xy = pd.DataFrame(X_train_os)\n",
    "Xy['TARGET'] = y_train_os\n",
    "Xy = Xy.drop_duplicates()\n",
    "y_train_os = Xy['TARGET'].to_numpy()\n",
    "X_train_os = Xy.drop(columns=['TARGET']).to_numpy()\n",
    "\n",
    "# --------------------------\n",
    "# Remove zero variance columns\n",
    "# --------------------------\n",
    "mask = X_train_os.var(axis=0) > 0\n",
    "X_train_os = X_train_os[:, mask]\n",
    "X_val_t = X_val_t[:, mask]\n",
    "\n",
    "print(f\"Training set shape: {X_train_os.shape}, Validation set shape: {X_val_t.shape}\")\n",
    "\n",
    "# --------------------------\n",
    "# Train LGBM\n",
    "# --------------------------\n",
    "clf = LGBMClassifier(\n",
    "    device=\"gpu\",\n",
    "    gpu_platform_id=1,\n",
    "    gpu_device_id=0,\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=10,\n",
    "    num_leaves=64,\n",
    "    min_child_samples=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=2.0,\n",
    "    reg_lambda=2.0,\n",
    "    scale_pos_weight=2,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(\n",
    "    X_train_os, y_train_os,\n",
    "    eval_set=[(X_val_t, y_val)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[early_stopping(50), log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Evaluation\n",
    "# --------------------------\n",
    "y_prob = clf.predict_proba(X_val_t)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_val, y_prob))\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_prob)\n",
    "print(\"PR AUC:\", auc(recall, precision))\n",
    "y_pred = (y_prob >= 0.1).astype(int)\n",
    "print(classification_report(y_val, y_pred, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# --------------------------\n",
    "# Process test data and predict\n",
    "# --------------------------\n",
    "print(\"Processing test data...\")\n",
    "test_df = process_data(\"transactions_test.csv\", is_test=True)\n",
    "X_test = test_df.copy()\n",
    "\n",
    "for col in categorical:\n",
    "    if col in X_test.columns:\n",
    "        print(f\"Column {col} has mixed types: {set(type(x) for x in X_test[col].dropna())}. Making it all string\")\n",
    "        X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "for col in numeric:\n",
    "    if col in X_test.columns:\n",
    "        print(f\"Column {col} has mixed types: {set(type(x) for x in X_test[col].dropna())}. Making it all float\")\n",
    "        X_test[col] = X_test[col].astype(float)\n",
    "\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "X_test_t = X_test_t[:, mask]\n",
    "\n",
    "y_test_prob = clf.predict_proba(X_test_t)[:,1]\n",
    "\n",
    "# --------------------------\n",
    "# Save predictions\n",
    "# --------------------------\n",
    "save_path = Path(\"predictions\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "version = 0\n",
    "while (save_path / f\"V{version}_LGBM.csv\").exists():\n",
    "    version += 1\n",
    "out_file = save_path / f\"V{version}_LGBM.csv\"\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"TX_ID\": pd.read_csv(\"transactions_test.csv\", low_memory=False)[\"TX_ID\"],\n",
    "    \"TX_FRAUD\": y_test_prob\n",
    "}).to_csv(out_file, index=False)\n",
    "\n",
    "print(f\"Saved test predictions to {out_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
